#!/usr/bin/env python3

"""Launch script for the chatbot services.

This script handles:
1. Starting required Docker services (Qdrant, OpenSearch)
2. Managing Ollama process and model loading
3. Checking service health
"""

2. Managing Ollama process and model loading    """Start Ollama service with GPU logging."""

3. Checking service health    print("🚀 Starting Ollama with log redirection...")

"""    Config.LOG_DIR.mkdir(parents=True, exist_ok=True)

    Config.GPU_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

import os    Config.GPU_LOG_PATH.write_text("")  # Clear old log

import subprocess    subprocess.Popen(f"ollama serve > {Config.GPU_LOG_PATH} 2>&1 &", shell=True)

import time    time.sleep(3)rch)

import requests2. Managing Ollama process and model loading

from typing import Optional3. Checking service health

from pathlib import Path"""

from dotenv import load_dotenv

import os

# Load environment variablesimport subprocess

load_dotenv()import time

import requests

# Configurationfrom typing import Optional

class Config:from pathlib import Path

    # Project pathsfrom dotenv import load_dotenv

    PROJECT_ROOT = Path(__file__).resolve().parent.parent

    LOG_DIR = PROJECT_ROOT / "logs"# Load environment variables

    GPU_LOG_PATH = Path.home() / ".ollama" / "gpu.log"load_dotenv()

    

    # Service URLs# Configuration

    OLLAMA_URL = "http://localhost:11434"class Config:

    QDRANT_URL = "http://localhost:6333"    # Project paths

    OPENSEARCH_URL = "http://localhost:9200"    PROJECT_ROOT = Path(__file__).resolve().parent.parent

        

    # Model configuration    # Service URLs

    DEFAULT_MODEL = "command-r7b"    OLLAMA_URL = "http://localhost:11434"

    OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY")    QDRANT_URL = "http://localhost:6333"

    OPENSEARCH_URL = "http://localhost:9200"

def run_command(command: str, check: bool = True) -> subprocess.CompletedProcess:    

    """Run a shell command and handle errors."""    # Model configuration

    try:    DEFAULT_MODEL = "command-r7b"

        result = subprocess.run(    OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY")

            command,    

            shell=True,    # Logging

            check=check,    LOG_DIR = PROJECT_ROOT / "logs"

            stdout=subprocess.PIPE,    GPU_LOG_PATH = Path.home() / ".ollama" / "gpu.log"

            stderr=subprocess.PIPE,

            text=True

        )def run_command(command: str, check: bool = True) -> subprocess.CompletedProcess:

        print(f"✅ Success: {command}")    """Run a shell command and handle errors."""

        if result.stdout:    try:

            print(result.stdout)        result = subprocess.run(

        return result            command,

    except subprocess.CalledProcessError as e:            shell=True,

        print(f"❌ Failed: {command}")            check=check,

        print(e.stderr)            stdout=subprocess.PIPE,

        raise            stderr=subprocess.PIPE,

            text=True

def wait_for_service(name: str, url: str, retries: int = 10, delay: int = 3) -> bool:        )

    """Wait for a service to become available."""        print(f"✅ Success: {command}")

    print(f"⏳ Waiting for {name} to be ready...")        if result.stdout:

    for i in range(retries):            print(result.stdout)

        try:        return result

            response = requests.get(url, timeout=2)    except subprocess.CalledProcessError as e:

            if response.ok and response.content:        print(f"❌ Failed: {command}")

                print(f"✅ {name} is running")        print(e.stderr)

                return True        raise

        except requests.exceptions.RequestException:

            print(f"🔄 Attempt {i+1}: {name} not ready yet...")

        time.sleep(delay)def wait_for_ollama(retries: int = 10, delay: int = 3) -> bool:

    print(f"❌ {name} did not respond after {retries * delay} seconds")    """Wait for Ollama service to become available."""

    return False    print("⏳ Waiting for Ollama to be ready...")

    for i in range(retries):

def wait_for_ollama(retries: int = 10, delay: int = 3) -> bool:        try:

    """Wait for Ollama service to become available."""            response = requests.get(f"{Config.OLLAMA_URL}/api/tags", timeout=2)

    print("⏳ Waiting for Ollama to be ready...")            if response.ok and "models" in response.json():

    for i in range(retries):                print("✅ Ollama is running")

        try:                return True

            response = requests.get(f"{Config.OLLAMA_URL}/api/tags", timeout=2)        except requests.exceptions.RequestException:

            if response.ok and "models" in response.json():            print(f"🔄 Attempt {i+1}: Ollama not ready yet...")

                print("✅ Ollama is running")        time.sleep(delay)

                return True    print(f"❌ Ollama did not respond after {retries * delay} seconds")

        except requests.exceptions.RequestException:    return False

            print(f"🔄 Attempt {i+1}: Ollama not ready yet...")

        time.sleep(delay)def check_gpu_status(timeout: int = 10) -> None:

    print(f"❌ Ollama did not respond after {retries * delay} seconds")    """Monitor Ollama's GPU detection from logs."""

    return False    print("🔍 Waiting for GPU status in Ollama log...")

    start_time = time.time()

def check_gpu_status(timeout: int = 10) -> None:    

    """Monitor Ollama's GPU detection from logs."""    # Ensure log directory exists

    print("🔍 Waiting for GPU status in Ollama log...")    Config.LOG_DIR.mkdir(parents=True, exist_ok=True)

    start_time = time.time()    

    while time.time() - start_time < timeout:    while time.time() - start_time < timeout:

        try:        try:

            log_content = Config.GPU_LOG_PATH.read_text()            log_content = Config.GPU_LOG_PATH.read_text()

            if "inference compute" in log_content.lower():            for line in log_content.splitlines():

                if 'name="' in log_content:                    if "inference compute" in line.lower():

                    gpu_name = log_content.split('name="')[1].split('"')[0]                        if 'name="' in line:

                    print(f"✅ GPU detected and in use: {gpu_name}")                            gpu_name = line.split('name="')[1].split('"')[0]

                else:                            print(f"✅ GPU detected and in use: {gpu_name}")

                    print("✅ GPU detected (name not found in log line).")                        else:

                return                            print("✅ GPU detected (name not found in log line).")

            if "no compatible GPUs" in log_content.lower():                        return

                print("⚠️ No compatible GPU found for local models.")                    if "no compatible GPUs" in line.lower():

                return                        print("⚠️ No compatible GPU found for local models.")

        except Exception as e:                        return

            print(f"❌ Could not read GPU log: {e}")        except Exception as e:

        time.sleep(1)            print(f"❌ Could not read GPU log: {e}")

    print("ℹ️ GPU status unclear. Check Ollama output manually.")        time.sleep(1)

    print("ℹ️ GPU status unclear. Check Ollama output manually.")

def start_ollama_with_logging():

    """Start Ollama service with GPU logging."""

    print("🚀 Starting Ollama with log redirection...")def is_model_available(model_name: str) -> bool:

    Config.LOG_DIR.mkdir(parents=True, exist_ok=True)    """Check if a model is already downloaded in Ollama."""

    Config.GPU_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)    try:

    Config.GPU_LOG_PATH.write_text("")  # Clear old log        response = requests.get(f"{Config.OLLAMA_URL}/api/tags", timeout=3)

    subprocess.Popen(f"ollama serve > {Config.GPU_LOG_PATH} 2>&1 &", shell=True)        if response.ok:

    time.sleep(3)            models = response.json().get("models", [])

            return any(model.get("name") == model_name for model in models)

def stop_ollama():    except requests.exceptions.RequestException:

    """Stop any running Ollama processes."""        pass

    print("🛑 Stopping any running Ollama processes...")    return False

    subprocess.run("pkill -x ollama", shell=True)

    time.sleep(2)

def pull_model(model_name: str) -> None:

def is_model_available(model_name: str) -> bool:    """Pull a model from Ollama if not already available."""

    """Check if a model is already downloaded in Ollama."""    if is_model_available(model_name):

    try:        print(f"✅ Model '{model_name}' is already available.")

        response = requests.get(f"{Config.OLLAMA_URL}/api/tags", timeout=3)        return

        if response.ok:

            models = response.json().get("models", [])    print(f"📦 Pulling model '{model_name}'...")

            return any(model.get("name") == model_name for model in models)    result = run_command(f"ollama pull {model_name}", check=False)

    except requests.exceptions.RequestException:    if result.returncode == 0:

        return False        print(f"✅ Model '{model_name}' pulled successfully.")

    return False    else:

        print(f"❌ Failed to pull model '{model_name}'")

def pull_model_if_missing(model_name: str) -> None:        print(result.stderr)

    """Pull a model if it's not already available."""

    if is_model_available(model_name):

        print(f"✅ Model '{model_name}' is already available.")def manage_ollama() -> None:

    else:    """Stop any running Ollama process and start a new one with logging."""

        print(f"📦 Pulling model '{model_name}'...")    print("🛑 Stopping any running Ollama processes...")

        result = subprocess.run(    subprocess.run("pkill -x ollama", shell=True)

            f"ollama pull {model_name}",    time.sleep(2)

            shell=True,

            stdout=subprocess.PIPE,    print("🚀 Starting Ollama with log redirection...")

            stderr=subprocess.PIPE,    # Clear old log

            text=True    open(GPU_LOG_PATH, "w").close()

        )    subprocess.Popen(f"ollama serve > {GPU_LOG_PATH} 2>&1 &", shell=True)

        if result.returncode == 0:    time.sleep(3)

            print(f"✅ Model '{model_name}' pulled successfully.")

        else:

            print(f"❌ Failed to pull model '{model_name}'")def main(model_name: Optional[str] = None):

            print(result.stderr)    """Main launch sequence."""

    model_name = model_name or Config.DEFAULT_MODEL

def main():    

    """Main launch sequence."""    try:

    model_name = Config.DEFAULT_MODEL        # 1. Start Docker services

        run_command("docker compose up -d")

    try:

        # Step 1: Start Docker services        # 2. Manage Ollama process

        run_command("docker compose up -d")        manage_ollama()

        if not model_name.endswith("-cloud"):

        # Step 2: Stop Ollama and restart with GPU check            check_gpu_status()

        stop_ollama()

        start_ollama_with_logging()        # 3. Wait for services

        if not model_name.endswith("-cloud"):        services = {

            check_gpu_status()            "Ollama": f"{Config.OLLAMA_URL}/api/tags",

            "Qdrant": "http://localhost:6333",

        # Step 3: Wait for services to be ready            "OpenSearch": "http://localhost:9200"

        services = {        }

            "Ollama": f"{Config.OLLAMA_URL}/api/tags",

            "Qdrant": Config.QDRANT_URL,        all_ready = True

            "OpenSearch": Config.OPENSEARCH_URL        for name, url in services.items():

        }            if not wait_for_service(name, url):

                        all_ready = False

        services_ready = all(wait_for_service(name, url) for name, url in services.items())

        # 4. Pull model if needed

        # Step 4: Pull model if needed        if all_ready:

        if services_ready:            pull_model(model_name)

            pull_model_if_missing(model_name)            print("🎉 All services are ready!")

            print("🎉 All services are up! Ready to launch chatbot.")        else:

            return True            print("🚫 Some services failed to start. Check the logs and try again.")

        else:            return 1

            print("🚫 Startup failed. Check logs and try again.")

            return False    except Exception as e:

        print(f"❌ Launch failed: {str(e)}")

    except Exception as e:        return 1

        print(f"❌ Launch failed: {str(e)}")

        return False    return 0



if __name__ == "__main__":

    main()if __name__ == "__main__":
    import sys
    sys.exit(main())